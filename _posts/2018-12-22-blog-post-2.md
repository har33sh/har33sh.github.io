---
title: 'Logistic Regression'
date: 2018-12-22
permalink: /posts/2018/12/blog-post-2/
tags:
  - Logistic Regression
  - Neural Network in 8 lines
---

# Logistic Regression
Logistic regression is algorithm for binary classification (True/False). Eg: Predict if the given image is Cat Image or not. Logistic Regression could also be said as Neural Network with single hidden layer.

Given x_i (*i* th sample in x), we want to predict y_i. in other words, we want  ```y_pred = P(y_i=1|x_i)```  Probabilty of y_i=1 for the given x_i.


## Preiction
``` y_pred = np.dot(w.T,x)+b ```
(*w* is some vector and *b* is a real number)
works in the case of linear Regression. Logistic regression needs value between [0,1]. So we add a function called sigmoid which returns value between [0,1].

```  y_pred = sigmoid(np.dot(w.T,x)+b) ``` <br>
where  ```  sigmoid(z) = 1/(1+np.exp(-z)) ```

If *z* is large then sigmoid(*z*) = (1/(1+0)) = 1 <br>
If *z* is small then sigmoid(*z*) = (1/(1+large_number)) ~ 0


## Cost Function
Cost Function is the metric used to measure how good the model is performing,

### Loss/Error Function :
Error for single sample *i* in the training set can be calculated as

 ```Loss(y_pred_i, y_i) = -1 * (y_i*np.log(y_pred_i) + (1-y_i)*np.log(1-y_pred_i)) ```

 If y_i=1, we want -log (y_pred_i) to be large (1) <br>
 If y_i=0, we want -log (1-y_pred_i) to be large (0) <br>

 ```Cost(w,b) = (1/m)*np.sum(Loss(y_pred,y))```
 where *m* is the size of training set.


## Gradient Descent
Gradient Descent is used to reduce the Cost function or Error of the model. To reduce the error, we compute the derivative from right to left of the model.

```
dw= 1/m * (np.dot(X,(y_pred-y).T))
db= 1/m * np.sum(y_pred-y)
```

we update the values of *w* and *b* by
'''
w = w - learning_rate * dw
b = b - learning_rate * db
'''

Prediction is called as Forward Propagation and Computing the Gradient Descent and updating the values of *w* and *b* is called as Reverse/Backward Propagation.

## Summary


```
//Forward Propagation
sigmoid(z) = 1/(1+np.exp(-z))
y_pred = sigmoid(np.dot(w.T,x)+b)

//Calculate Loss
Loss(y_pred, y) = -1*(y*np.log(y_pred) + (1-y)*np.log(1-y_pred)
Cost(w,b) = (1/m)*np.sum(Loss(y_pred,y))

//Reverse Propagation
dw= 1/m * (np.dot(X,(y_pred-y).T))
db= 1/m * np.sum(y_pred-y)

//Update the values
w = w - learning_rate * dw
b = b - learning_rate * db
```
The above code can be run for multiple iterations until the error of the model reduces
